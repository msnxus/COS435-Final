{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "tensor(7)\n",
      "[tensor(8), tensor(3), tensor(3), tensor(3), tensor(3), tensor(2), tensor(5), tensor(3), tensor(6), tensor(0)]\n",
      "[tensor([17.5183]), tensor([25.9750]), tensor([27.6357]), tensor([-70.]), tensor([-70.]), tensor([36.6212]), tensor([46.1148]), tensor([-50.]), tensor([56.1928]), tensor([66.2889])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j0/vdrkjwcn4n3dtw7yb25zgj7r0000gn/T/ipykernel_66101/1408183713.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  current_vertex_onehot = F.one_hot(torch.tensor(current_vertex), num_classes=env.n_vertices).float().squeeze(0)\n",
      "/var/folders/j0/vdrkjwcn4n3dtw7yb25zgj7r0000gn/T/ipykernel_66101/1408183713.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  current_vertex_onehot = F.one_hot(torch.tensor(current_vertex), num_classes=env.n_vertices).float().squeeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 | Total Reward: 86.35 | Loss: 1635.3099\n",
      "tensor(3)\n",
      "[tensor(5), tensor(5), tensor(5), tensor(5), tensor(5), tensor(8), tensor(5), tensor(5), tensor(5), tensor(5)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([30.]), tensor([-70.]), tensor([-70.]), tensor([-70.]), tensor([-70.])]\n",
      "Episode 1 | Total Reward: -650.00 | Loss: 74802.7109\n",
      "tensor(4)\n",
      "[tensor(2), tensor(2), tensor(2), tensor(2), tensor(6), tensor(2), tensor(2), tensor(7), tensor(6), tensor(8)]\n",
      "[tensor([20.]), tensor([20.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([30.]), tensor([-70.]), tensor([40.])]\n",
      "Episode 2 | Total Reward: -360.00 | Loss: 27376.1953\n",
      "tensor(4)\n",
      "[tensor(6), tensor(6), tensor(6), tensor(6), tensor(2), tensor(4), tensor(6), tensor(4), tensor(2), tensor(4)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([30.]), tensor([40.]), tensor([-60.]), tensor([-60.]), tensor([-60.]), tensor([-60.])]\n",
      "Episode 3 | Total Reward: -490.00 | Loss: 23758.4922\n",
      "tensor(2)\n",
      "[tensor(4), tensor(4), tensor(4), tensor(4), tensor(4), tensor(4), tensor(4), tensor(4), tensor(4), tensor(4)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 4 | Total Reward: -800.00 | Loss: 44084.8828\n",
      "tensor(2)\n",
      "[tensor(4), tensor(4), tensor(4), tensor(4), tensor(4), tensor(4), tensor(4), tensor(4), tensor(4), tensor(4)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 5 | Total Reward: -800.00 | Loss: 32878.1680\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([20.]), tensor([20.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 6 | Total Reward: -600.00 | Loss: 2561.6643\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 7 | Total Reward: -800.00 | Loss: 33188.9258\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 8 | Total Reward: -800.00 | Loss: 27653.7598\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 9 | Total Reward: -800.00 | Loss: 11356.5107\n",
      "tensor(1)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-90.]), tensor([-90.]), tensor([-90.]), tensor([-90.]), tensor([-90.]), tensor([-90.]), tensor([-90.]), tensor([-90.]), tensor([-90.]), tensor([-90.])]\n",
      "Episode 10 | Total Reward: -900.00 | Loss: 30357.0547\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 11 | Total Reward: -800.00 | Loss: 14513.9873\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 12 | Total Reward: -800.00 | Loss: 28235.0781\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 13 | Total Reward: -800.00 | Loss: 4074.9963\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 14 | Total Reward: -800.00 | Loss: 11403.6855\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 15 | Total Reward: -800.00 | Loss: 10438.1084\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 16 | Total Reward: -800.00 | Loss: 3821.9492\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 17 | Total Reward: -800.00 | Loss: 13117.8389\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 18 | Total Reward: -800.00 | Loss: 3883.8125\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 19 | Total Reward: -800.00 | Loss: 5171.0776\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 20 | Total Reward: -800.00 | Loss: 4033.4963\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 21 | Total Reward: -800.00 | Loss: 4531.0991\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 22 | Total Reward: -800.00 | Loss: 3606.6240\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 23 | Total Reward: -800.00 | Loss: 8527.0332\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 24 | Total Reward: -800.00 | Loss: 3264.0735\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 25 | Total Reward: -800.00 | Loss: 19488.0059\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 26 | Total Reward: -800.00 | Loss: 11306.5850\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 27 | Total Reward: -800.00 | Loss: 5885.9727\n",
      "tensor(1)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-90.]), tensor([-90.]), tensor([-90.]), tensor([-90.]), tensor([-90.]), tensor([-90.]), tensor([-90.]), tensor([-90.]), tensor([-90.]), tensor([-90.])]\n",
      "Episode 28 | Total Reward: -900.00 | Loss: 33319.6523\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 29 | Total Reward: -800.00 | Loss: 11827.3750\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 30 | Total Reward: -800.00 | Loss: 6386.1631\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 31 | Total Reward: -800.00 | Loss: 21777.1445\n",
      "tensor(1)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-90.]), tensor([-90.]), tensor([-90.]), tensor([-90.]), tensor([-90.]), tensor([-90.]), tensor([-90.]), tensor([-90.]), tensor([-90.]), tensor([-90.])]\n",
      "Episode 32 | Total Reward: -900.00 | Loss: 86825.0859\n",
      "tensor(1)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-90.]), tensor([-90.]), tensor([-90.]), tensor([-90.]), tensor([-90.]), tensor([-90.]), tensor([-90.]), tensor([-90.]), tensor([-90.]), tensor([-90.])]\n",
      "Episode 33 | Total Reward: -900.00 | Loss: 54301.7969\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 34 | Total Reward: -800.00 | Loss: 21914.6602\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 35 | Total Reward: -800.00 | Loss: 15232.1602\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 36 | Total Reward: -800.00 | Loss: 7331.1152\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 37 | Total Reward: -800.00 | Loss: 29418.6328\n",
      "tensor(1)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-90.]), tensor([-90.]), tensor([-90.]), tensor([-90.]), tensor([-90.]), tensor([-90.]), tensor([-90.]), tensor([-90.]), tensor([-90.]), tensor([-90.])]\n",
      "Episode 38 | Total Reward: -900.00 | Loss: 57811.1133\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 39 | Total Reward: -800.00 | Loss: 5542.5605\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 40 | Total Reward: -800.00 | Loss: 7217.5894\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 41 | Total Reward: -800.00 | Loss: 15570.5176\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 42 | Total Reward: -800.00 | Loss: 6170.6006\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 43 | Total Reward: -800.00 | Loss: 6685.1865\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 44 | Total Reward: -800.00 | Loss: 3554.7097\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 45 | Total Reward: -800.00 | Loss: 7082.7637\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 46 | Total Reward: -800.00 | Loss: 15802.3008\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 47 | Total Reward: -800.00 | Loss: 5907.9175\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 48 | Total Reward: -800.00 | Loss: 8001.8813\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 49 | Total Reward: -800.00 | Loss: 8455.3350\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 50 | Total Reward: -800.00 | Loss: 4529.3652\n",
      "âœ… Saved model at episode 50\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 51 | Total Reward: -800.00 | Loss: 8923.0391\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 52 | Total Reward: -800.00 | Loss: 4891.3120\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 53 | Total Reward: -800.00 | Loss: 5918.2329\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 54 | Total Reward: -800.00 | Loss: 4839.8706\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 55 | Total Reward: -800.00 | Loss: 4628.6411\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 56 | Total Reward: -800.00 | Loss: 3137.2212\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 57 | Total Reward: -800.00 | Loss: 4872.3228\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 58 | Total Reward: -800.00 | Loss: 4482.4756\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 59 | Total Reward: -800.00 | Loss: 7045.8076\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 60 | Total Reward: -800.00 | Loss: 3532.8259\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 61 | Total Reward: -800.00 | Loss: 3131.1577\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 62 | Total Reward: -800.00 | Loss: 3123.2512\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 63 | Total Reward: -800.00 | Loss: 3117.0830\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 64 | Total Reward: -800.00 | Loss: 4648.6855\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 65 | Total Reward: -800.00 | Loss: 4192.1685\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 66 | Total Reward: -800.00 | Loss: 3564.5437\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 67 | Total Reward: -800.00 | Loss: 4349.2080\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 68 | Total Reward: -800.00 | Loss: 4170.9595\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 69 | Total Reward: -800.00 | Loss: 3217.1157\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 70 | Total Reward: -800.00 | Loss: 3357.5557\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 71 | Total Reward: -800.00 | Loss: 3267.4290\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 72 | Total Reward: -800.00 | Loss: 3633.9875\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 73 | Total Reward: -800.00 | Loss: 4795.7866\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 74 | Total Reward: -800.00 | Loss: 3794.2087\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 75 | Total Reward: -800.00 | Loss: 3196.4363\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 76 | Total Reward: -800.00 | Loss: 3270.4744\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 77 | Total Reward: -800.00 | Loss: 3093.3086\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 78 | Total Reward: -800.00 | Loss: 3466.9333\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 79 | Total Reward: -800.00 | Loss: 3751.3071\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 80 | Total Reward: -800.00 | Loss: 3094.6592\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 81 | Total Reward: -800.00 | Loss: 4295.1431\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 82 | Total Reward: -800.00 | Loss: 3169.5391\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 83 | Total Reward: -800.00 | Loss: 3284.2024\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 84 | Total Reward: -800.00 | Loss: 3504.1333\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 85 | Total Reward: -800.00 | Loss: 3264.6082\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 86 | Total Reward: -800.00 | Loss: 3152.5320\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 87 | Total Reward: -800.00 | Loss: 5941.2744\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 88 | Total Reward: -800.00 | Loss: 3453.1589\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 89 | Total Reward: -800.00 | Loss: 3444.9639\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 90 | Total Reward: -800.00 | Loss: 3451.2695\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 91 | Total Reward: -800.00 | Loss: 3070.0361\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 92 | Total Reward: -800.00 | Loss: 3594.6941\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 93 | Total Reward: -800.00 | Loss: 3241.8875\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 94 | Total Reward: -800.00 | Loss: 3158.4639\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 95 | Total Reward: -800.00 | Loss: 3149.1772\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 96 | Total Reward: -800.00 | Loss: 3299.5293\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n",
      "Episode 97 | Total Reward: -800.00 | Loss: 3959.0505\n",
      "tensor(2)\n",
      "[tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
      "[tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.]), tensor([-80.])]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 114\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mfor\u001b[39;00m i, (state_tensor, path_tensor, visited_tensor, one_hot_tensor) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(states):\n\u001b[1;32m    113\u001b[0m     logits, value \u001b[39m=\u001b[39m model(state_tensor, path_tensor, visited_tensor, one_hot_tensor)\n\u001b[0;32m--> 114\u001b[0m     dist \u001b[39m=\u001b[39m Categorical(logits\u001b[39m=\u001b[39;49mlogits)\n\u001b[1;32m    115\u001b[0m     new_log_probs\u001b[39m.\u001b[39mappend(dist\u001b[39m.\u001b[39mlog_prob(actions[i]))\n\u001b[1;32m    116\u001b[0m     new_values\u001b[39m.\u001b[39mappend(value\u001b[39m.\u001b[39msqueeze())\n",
      "File \u001b[0;32m~/.virtualenvs/caiden1/lib/python3.10/site-packages/torch/distributions/categorical.py:66\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_events \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param\u001b[39m.\u001b[39msize()[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     65\u001b[0m batch_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param\u001b[39m.\u001b[39mndimension() \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mSize()\n\u001b[0;32m---> 66\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(batch_shape, validate_args\u001b[39m=\u001b[39;49mvalidate_args)\n",
      "File \u001b[0;32m~/.virtualenvs/caiden1/lib/python3.10/site-packages/torch/distributions/distribution.py:60\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# skip checking lazily-constructed args\u001b[39;00m\n\u001b[1;32m     59\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, param)\n\u001b[0;32m---> 60\u001b[0m valid \u001b[39m=\u001b[39m constraint\u001b[39m.\u001b[39;49mcheck(value)\n\u001b[1;32m     61\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid\u001b[39m.\u001b[39mall():\n\u001b[1;32m     62\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     63\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected parameter \u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(value\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut found invalid values:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m     )\n",
      "File \u001b[0;32m~/.virtualenvs/caiden1/lib/python3.10/site-packages/torch/distributions/constraints.py:213\u001b[0m, in \u001b[0;36m_IndependentConstraint.check\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    211\u001b[0m     expected \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_constraint\u001b[39m.\u001b[39mevent_dim \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreinterpreted_batch_ndims\n\u001b[1;32m    212\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected value.dim() >= \u001b[39m\u001b[39m{\u001b[39;00mexpected\u001b[39m}\u001b[39;00m\u001b[39m but got \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 213\u001b[0m result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mreshape(result\u001b[39m.\u001b[39mshape[:result\u001b[39m.\u001b[39;49mdim() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreinterpreted_batch_ndims] \u001b[39m+\u001b[39m (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,))\n\u001b[1;32m    214\u001b[0m result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mall(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    215\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ðŸ§  PPO Training for TrafficEnv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from simulator import TrafficEnv  # Make sure this points to your .py with TrafficEnv\n",
    "from model import TrafficNet  # Make sure this points to your GNN-like TrafficNet\n",
    "def print_state(obs, n):\n",
    "    traffic_tensor, dijkstra_tensor = obs\n",
    "    for t in range(n):\n",
    "        print('-------------------------')\n",
    "        print('t =', t)\n",
    "        print(traffic_tensor[:, :, t])\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Instantiate environment and model\n",
    "env = TrafficEnv(\"traffic_maps\", time_per_step=10, max_steps=10)  # Replace with actual path\n",
    "obs_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "\n",
    "# Create model\n",
    "model = TrafficNet(\n",
    "    n_vertices=env.n_vertices,\n",
    "    n_timesteps=env.n_timesteps,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "clip_epsilon = 0.2\n",
    "gamma = 0.99\n",
    "save_every = 50  # Save model every N episodes\n",
    "\n",
    "# Training loop parameters\n",
    "num_episodes = 10000\n",
    "max_steps = env.max_steps\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs, _ = env.reset()\n",
    "    traffic_tensor = torch.tensor(obs[0], dtype=torch.float32, device=device)\n",
    "    next_paths = torch.tensor(obs[1], dtype=torch.float32, device=device)\n",
    "    visited = torch.tensor(obs[2], dtype=torch.float32, device=device)\n",
    "    current_vertex = torch.tensor([obs[3]], dtype=torch.long, device=device)\n",
    "\n",
    "    current_vertex_onehot = F.one_hot(torch.tensor(current_vertex), num_classes=env.n_vertices).float().squeeze(0)\n",
    "\n",
    "    # print_state((traffic_tensor, next_paths), traffic_tensor.size(0))\n",
    "\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "    states = []\n",
    "    actions = []\n",
    "\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "\n",
    "\n",
    "        action_logits, state_value = model(traffic_tensor, next_paths, visited.float(), current_vertex_onehot)\n",
    "        dist = Categorical(logits=action_logits)\n",
    "        action = dist.sample()\n",
    "\n",
    "        new_obs, reward, done, truncated, _ = env.step(action.item())\n",
    "        traffic_tensor = torch.tensor(new_obs[0], dtype=torch.float32, device=device)\n",
    "        next_paths = torch.tensor(new_obs[1], dtype=torch.float32, device=device)\n",
    "        visited = torch.tensor(new_obs[2], dtype=torch.float32, device=device)\n",
    "        current_vertex = torch.tensor([new_obs[3]], dtype=torch.long, device=device)\n",
    "        current_vertex_onehot = F.one_hot(torch.tensor(current_vertex), num_classes=env.n_vertices).float().squeeze(0)\n",
    "        visited = visited.float()\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        states.append((traffic_tensor.clone(), next_paths.clone(), visited.clone(), current_vertex_onehot.clone()))\n",
    "        actions.append(action)\n",
    "        log_probs.append(dist.log_prob(action))\n",
    "        values.append(state_value.squeeze())\n",
    "        rewards.append(torch.tensor([reward], dtype=torch.float32, device=device))\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        traffic_tensor = torch.tensor(new_obs[0], dtype=torch.float32, device=device)\n",
    "        next_paths = torch.tensor(new_obs[1], dtype=torch.float32, device=device)\n",
    "    print(env.visited_vertices.sum())\n",
    "\n",
    "    # Compute returns and advantages\n",
    "    returns = []\n",
    "    G = 0\n",
    "    print(actions)\n",
    "    print(rewards)\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    returns = torch.cat(returns).detach()\n",
    "    values = torch.stack(values).detach()\n",
    "    log_probs = torch.stack(log_probs)\n",
    "    actions = torch.stack(actions)\n",
    "\n",
    "    advantages = (returns - values).detach()\n",
    "\n",
    "    # PPO loss\n",
    "    for _ in range(4):  # Multiple epochs\n",
    "        new_log_probs = []\n",
    "        new_values = []\n",
    "\n",
    "        for i, (state_tensor, path_tensor, visited_tensor, one_hot_tensor) in enumerate(states):\n",
    "            logits, value = model(state_tensor, path_tensor, visited_tensor, one_hot_tensor)\n",
    "            dist = Categorical(logits=logits)\n",
    "            new_log_probs.append(dist.log_prob(actions[i]))\n",
    "            new_values.append(value.squeeze())\n",
    "\n",
    "        new_log_probs = torch.stack(new_log_probs)\n",
    "        new_values = torch.stack(new_values)\n",
    "\n",
    "        ratio = (new_log_probs - log_probs.detach()).exp()\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * advantages\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "        value_loss = (returns - new_values).pow(2).mean()\n",
    "        entropy = dist.entropy().mean()\n",
    "        loss = policy_loss + 0.5 * value_loss - 0.1 * entropy\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Episode {episode} | Total Reward: {total_reward:.2f} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Save model every N episodes\n",
    "    if episode % save_every == 0 and episode > 0:\n",
    "        torch.save(model.state_dict(), f\"ppo_traffic_model_ep{episode}.pt\")\n",
    "        print(f\"âœ… Saved model at episode {episode}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caidenkiani/Desktop/COS_courses/COS435/COS435-Final/trainer_dql.py:148: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  actions = []\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrainer_dql\u001b[39;00m \u001b[39mimport\u001b[39;00m train\n\u001b[0;32m----> 2\u001b[0m early_stop, reward_list \u001b[39m=\u001b[39m train(num_steps\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, mini_batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m, ppo_epochs\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, threshold\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[0;32m~/Desktop/COS_courses/COS435/COS435-Final/trainer_dql.py:167\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(num_steps, mini_batch_size, ppo_epochs, threshold)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39m# Collect samples under the current policy\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2048\u001b[39m):\n\u001b[0;32m--> 167\u001b[0m     dist, value \u001b[39m=\u001b[39m policy_net(states), value_net(states)\n\u001b[1;32m    169\u001b[0m     action \u001b[39m=\u001b[39m dist\u001b[39m.\u001b[39msample()\n\u001b[1;32m    170\u001b[0m     obs, reward, done, _, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(\u001b[39mint\u001b[39m(action\u001b[39m.\u001b[39mitem()))  \u001b[39m# Ensure action is an int\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/caiden1/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/COS_courses/COS435/COS435-Final/trainer_dql.py:52\u001b[0m, in \u001b[0;36mPolicyNetwork.forward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, state):\n\u001b[0;32m---> 52\u001b[0m   h1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf1(state))\n\u001b[1;32m     53\u001b[0m   h2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf2(h1))\n\u001b[1;32m     54\u001b[0m   h3 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf3(h2))\n",
      "File \u001b[0;32m~/.virtualenvs/caiden1/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.virtualenvs/caiden1/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "from trainer_dql import train\n",
    "early_stop, reward_list = train(num_steps=100, mini_batch_size=16, ppo_epochs=4, threshold=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cos333",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
