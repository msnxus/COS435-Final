{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from new_env import CityEnv\n",
    "import gym\n",
    "import numpy as np\n",
    "from poly_matrix import polynomial, create_poly_matrix\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "N = 10\n",
    "k = 3\n",
    "T = 24\n",
    "poly_matrix = create_poly_matrix(N, k, T)\n",
    "\n",
    "class CustomResetWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, destinations, start_time, start_vertex):\n",
    "        super().__init__(env)\n",
    "        self.destinations = destinations\n",
    "        self.start_time = start_time\n",
    "        self.start_vertex = start_vertex\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        # Pass the custom arguments to the environment's reset method\n",
    "        return self.env.reset(\n",
    "            destinations=self.destinations,\n",
    "            start_time=self.start_time,\n",
    "            start_vertex=self.start_vertex,\n",
    "            **kwargs\n",
    "        )\n",
    "    \n",
    "# Create the environment\n",
    "env = CityEnv(poly_matrix=poly_matrix, N = N, time_horizon = T)\n",
    "destinations = np.array([0, 1, 1, 0, 1, 0, 1, 0, 1, 0])\n",
    "start_time = 0\n",
    "start_vertex = 4\n",
    "env = CustomResetWrapper(env, destinations=destinations, start_time=start_time, start_vertex=start_vertex)\n",
    "\n",
    "# Initialize the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=10000, progress_bar=True)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_city_env\")\n",
    "\n",
    "# Load the model (optional)\n",
    "model = PPO.load(\"ppo_city_env\")\n",
    "\n",
    "# Evaluate the model\n",
    "obs = env.reset()\n",
    "for _ in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "def print_state(obs, n):\n",
    "    traffic_tensor, dijkstra_tensor = obs\n",
    "    for t in range(n):\n",
    "        print('-------------------------')\n",
    "        print('t =', t)\n",
    "        print(traffic_tensor[:, :, t])\n",
    "\n",
    "\n",
    "def compute_returns(rewards, gamma, last_value):\n",
    "    returns = []\n",
    "    R = last_value\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "\n",
    "def train_ppo(\n",
    "    traffic_dir,\n",
    "    total_episodes=500,\n",
    "    gamma=0.99,\n",
    "    clip_epsilon=0.2,\n",
    "    update_epochs=4,\n",
    "    batch_size=5,\n",
    "    lr=2.5e-4\n",
    "):\n",
    "    env = CityEnv(poly_matrix=poly_matrix, N = N, time_horizon = T)\n",
    "    model = TrafficNet(env.n_vertices, env.n_timesteps)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for episode in range(total_episodes):\n",
    "        traffic_tensor, next_paths = env.reset()[0]\n",
    "        \n",
    "        traffic_tensor = torch.tensor(traffic_tensor, dtype=torch.float32)\n",
    "        next_paths = torch.tensor(next_paths, dtype=torch.float32)\n",
    "\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        states = []\n",
    "        actions = []\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            logits, value = model(traffic_tensor, next_paths)\n",
    "            dist = Categorical(logits=logits)\n",
    "            action = dist.sample()\n",
    "\n",
    "            (next_traffic, next_paths), reward, done, _, _ = env.step(action.item())\n",
    "            next_traffic = torch.tensor(next_traffic, dtype=torch.float32)\n",
    "            next_paths = torch.tensor(next_paths, dtype=torch.float32)\n",
    "\n",
    "            log_probs.append(dist.log_prob(action))\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor(reward, dtype=torch.float32))\n",
    "            actions.append(action)\n",
    "            states.append((traffic_tensor, next_paths))\n",
    "\n",
    "            traffic_tensor = next_traffic\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _, last_value = model(traffic_tensor, next_paths)\n",
    "        returns = compute_returns(rewards, gamma, last_value)\n",
    "\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        values = torch.stack(values)\n",
    "        returns = torch.stack(returns)\n",
    "        advantages = returns - values.detach()\n",
    "\n",
    "        for _ in range(update_epochs):\n",
    "            for i in range(0, len(states), batch_size):\n",
    "                batch = states[i:i + batch_size]\n",
    "                batch_actions = actions[i:i + batch_size]\n",
    "                batch_advantages = advantages[i:i + batch_size]\n",
    "                batch_returns = returns[i:i + batch_size]\n",
    "                batch_log_probs = log_probs[i:i + batch_size]\n",
    "\n",
    "                new_log_probs = []\n",
    "                new_values = []\n",
    "\n",
    "                for (traffic_tensor, next_paths), action in zip(batch, batch_actions):\n",
    "                    logits, value = model(traffic_tensor, next_paths)\n",
    "                    dist = Categorical(logits=logits)\n",
    "                    new_log_probs.append(dist.log_prob(action))\n",
    "                    new_values.append(value)\n",
    "\n",
    "                new_log_probs = torch.stack(new_log_probs)\n",
    "                new_values = torch.stack(new_values)\n",
    "                ratio = (new_log_probs - batch_log_probs).exp()\n",
    "\n",
    "                surr1 = ratio * batch_advantages\n",
    "                surr2 = torch.clamp(ratio, 1.0 - clip_epsilon, 1.0 + clip_epsilon) * batch_advantages\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                value_loss = nn.functional.mse_loss(new_values, batch_returns)\n",
    "\n",
    "                loss = policy_loss + 0.5 * value_loss\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        print(f\"Episode {episode+1}/{total_episodes}, Total reward: {sum(rewards).item():.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
